[BASIC]
# the file of input ontology; mandatory; you can also set it as a projected ontology
ontology_file = ./../output_files/cw-data-reasoned.owl
# ontology_file = ./cache/projection.ttl

# the output director for the embedding; if not set, it uses the default: $cache_dir/output
embedding_dir = ./output_embedding/cw_data.embeddings

[DOCUMENT]

# cache directory for storing files; if not set, it creates a default: ./cache/
cache_dir = ./cache/

# use or not use the projected ontology
# default: no
ontology_projection = yes



# reasoning for extract axioms: hermit or elk or none; none denotes not using reasoning.
# axiom_reasoner = hermit
# axiom_reasoner = elk
axiom_reasoner = none

# the seed entities for generating the walks
# default: all the named classes and instances cached in cache/entities.txt
# comment it if the default python program is called to extract all classes and individuals as the seed entities
# pre_entity_file = ./cache/entities.txt

# the annotations and axioms can pre-calculated and saved in the some directory (e.g., the cache directory)
# OWL2Vec will use the pre-calculated files if set, or it will extract them by default
# comment them if the default python program is called to extract annotations and axioms
# pre_annotation_file = ./cache/annotations.txt
# pre_axiom_file = ./cache/axioms.txt

# walker and walk_depth must be set
# random walk or random walk with Weisfeiler-Lehman (wl) subtree kernel
walker = random
# walker = wl
walk_depth = 4

# use URI/Literal/Mixture document (yes or no)
# they can be over witten by the command line parameters
URI_Doc = yes
Lit_Doc = yes
Mix_Doc = yes

# the type for generating the mixture document (all or random)
# works when Mix_Doc is set to yes
# Mix_Type = all
Mix_Type = random

[MODEL]

# the directory of the pre-trained language model
# default: without pre-training
# comment it if no pre-training is needed
#pre_train_model = ~/w2v_model/enwiki_model/word2vec_gensim

# the size for embedding
# it is set to the size of the pre-trained model if it is adopted
embed_size = 100

# number of iterations in training the language model
iteration = 10

# for training the language model without pre-training
window = 5
min_count = 1
negative = 25
seed = 42

# epoch for fine-tuning the pre-trained model
epoch = 100
